import logging
import nltk
from rouge import Rouge
from openai import OpenAI
from tqdm import tqdm

LLM_MODEL = "gpt-4o-mini-2024-07-18"
client = OpenAI(api_key="")
nltk.download("wordnet")
rouge = Rouge()


class GECScoreDetector(object):
    def __init__(self):
        pass

    def chat_with_gpt4o(self, prompt, model):
        """
        Interacts with GPT-4o or another LLM to generate text.

        Args:
            prompt (str): Input text to prompt the LLM.
            model (str): The LLM model to use.

        Returns:
            str: The response generated by the LLM.
        """
        try:
            completion = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": prompt},
                ],
                temperature=0.01,
            )
            return completion.choices[0].message.content
        except Exception as e:
            logging.error(f"Error during LLM interaction: {e}")
            return None

    def compute_score(self, text):
        prompt = f"Correct the grammar errors in the following text: {text}\nCorrected text:"
        # prompt = f"请修正以下文本中的语法错误：{text}\n修正后的文本："

        gec_text = self.chat_with_gpt4o(prompt, LLM_MODEL)
        if not gec_text:
            return {f"Error processing text: {text}"}

        try:
            rouge_score = rouge.get_scores(text, gec_text, avg=True)
            rouge2_score = rouge_score['rouge-2']['f']
        except Exception as rouge_error:
            logging.warning(f"Failed to compute Rouge score: {rouge_error}")

        return rouge2_score

    def batch_inference(self, texts: list) -> list:
        # but in the view of input to neural model, it's actually not a batch.
        predictions = []
        for text in tqdm(texts):
            predictions.append(self.compute_score(text))
        return predictions
